{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pdSQBdNuEQzC"},"outputs":[],"source":["from google.colab import drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35282,"status":"ok","timestamp":1680159566853,"user":{"displayName":"Amir Hosen","userId":"03445828835581704218"},"user_tz":-360},"id":"RABWYF-vEpDW","outputId":"4525c019-0fcc-4449-ef38-fb12b0fb1374"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Necessary Libraries"],"metadata":{"id":"paKJ9guqtDGC"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.layers import Input, Embedding, LSTM, Dense\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from keras.models import Model\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import TfidfTransformer\n","import nltk\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","import string\n","import re\n","from sklearn.feature_extraction.text import CountVectorizer\n","import seaborn as sns"],"metadata":{"id":"Qhfji7Gsbwwu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Connect datasets"],"metadata":{"id":"n4JcZRp9tNkt"}},{"cell_type":"code","source":["#dataset connection\n","df = pd.read_csv('/content/drive/MyDrive/Thesis/PersonalityPrediction/Datasets/our_dataset.csv')"],"metadata":{"id":"EZXNyrLmbxqJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJxYYOoT3BQr","executionInfo":{"status":"ok","timestamp":1680160064057,"user_tz":-360,"elapsed":874,"user":{"displayName":"Amir Hosen","userId":"03445828835581704218"}},"outputId":"59383c87-0ec5-499f-f620-ab7a20f7a753"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(19998, 6)\n"]}]},{"cell_type":"code","source":["print(df.head());"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"guP5uJWoB0Si","executionInfo":{"status":"ok","timestamp":1680160067480,"user_tz":-360,"elapsed":14,"user":{"displayName":"Amir Hosen","userId":"03445828835581704218"}},"outputId":"d0f7b7de-6187-4b29-9aef-b3854784e176"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                              STATUS  cEXT  cNEU  cAGR  cCON  \\\n","0  I'm freakin' distracted by a stupid video my 5...     0     1     0     0   \n","1  You know what's an interesting side effect of ...     0     0     1     0   \n","2  So I broke the glass to my coffee table when I...     0     0     1     0   \n","3  why tf did I just now remember etsy definitely...     1     1     0     1   \n","4  this makes Reigen chuckle, a grin drawing itse...     0     0     1     1   \n","\n","   cOPN  \n","0     1  \n","1     1  \n","2     1  \n","3     1  \n","4     1  \n"]}]},{"cell_type":"markdown","source":["Data Preprocessing"],"metadata":{"id":"uDZHpbkQAGnq"}},{"cell_type":"code","source":["# download dependencies\n","nltk.download('stopwords')\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1KXmzST9085","executionInfo":{"status":"ok","timestamp":1680159610253,"user_tz":-360,"elapsed":587,"user":{"displayName":"Amir Hosen","userId":"03445828835581704218"}},"outputId":"363af57c-f8e2-4e7f-cc13-fedd5bb7a19c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# stopwords in english\n","stop_words = stopwords.words('english')"],"metadata":{"id":"ITHEMvZWa_2F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Data Cleaning"],"metadata":{"id":"uqmRIEhJa1i3"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# create a new list to store the filtered sentences\n","new_text = []  \n","\n","for index, row in df.iterrows():\n","    filter_sentence = []\n","    sentence = row['STATUS']\n","\n","    # remove punctuation\n","    sentence = re.sub(r'[^\\w\\s]', '', sentence) \n","\n","    # tokenize\n","    words = nltk.word_tokenize(sentence)  \n","\n","    # remove stopwords\n","    words = [w for w in words if not w in stop_words] \n","\n","    # lemmatization\n","    for word in words:\n","        filter_sentence.append(lemmatizer.lemmatize(word))  \n","\n","    # add the filtered sentence to the new list\n","    new_text.append(filter_sentence)  \n","\n","# replace the 'TEXT' column with the new list\n","df['STATUS'] = new_text  \n"],"metadata":{"id":"l1DyD4kLazei"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create dependent and independent variables"],"metadata":{"id":"ga_i7EVeIbBE"}},{"cell_type":"code","source":["# input variable and output variable\n","text_data = df['TEXT']\n","personality_traits = df['cEXT']"],"metadata":{"id":"B9uXi_yQBUhT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Split the dataset into feature variable X and target variable y\n","X = df['STATUS'].apply(str)\n","y = df['cEXT']\n","\n","# Convert the text data into numerical features using TF-IDF\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(X)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train the SVM classifier\n","clf = SVC(kernel='linear')\n","clf.fit(X_train, y_train)\n","\n","# Evaluate the performance of the model on the testing set\n","y_pred = clf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print('Accuracy:', round(accuracy, 2)*100)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TARJuL3OU0eX","executionInfo":{"status":"ok","timestamp":1680160534212,"user_tz":-360,"elapsed":66666,"user":{"displayName":"Amir Hosen","userId":"03445828835581704218"}},"outputId":"4f22429e-7b4a-4eac-a7db-382ec3ccaac4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 83.0\n"]}]},{"cell_type":"markdown","source":["Labling with Model"],"metadata":{"id":"wSrY95wjtwwC"}},{"cell_type":"code","source":["import glob, os\n","\n","# will label dataset\n","df = pd.read_csv(\"/content/drive/MyDrive/Thesis/PersonalityPrediction/Datasets/will_label_20000.csv\")\n","df.shape\n","  \n","sEXT = []\n","sNEU = []\n","sAGR = []\n","sCON = []\n","sOPN = []\n","\n","for text in df['TEXT'].astype(str):\n","  new_data = [text]\n","  new_sequences = tokenizer.texts_to_sequences(new_data)\n","  new_padded_sequences = pad_sequences(new_sequences, maxlen=max_sequence_length)\n","  predictions = model.predict(new_padded_sequences)\n","\n","  sEXT.append(round(float(predictions[0]), 2));\n","  sNEU.append(round(float(predictions[1]), 2));\n","  sAGR.append(round(float(predictions[2]), 2));\n","  sCON.append(round(float(predictions[3]), 2));\n","  sOPN.append(round(float(predictions[4]), 2));\n","\n","df['sEXT'] = sEXT\n","df['sNEU'] = sNEU\n","df['sAGR'] = sAGR\n","df['sCON'] = sCON\n","df['sOPN'] = sOPN\n","\n","df.to_csv(f\"/content/drive/MyDrive/Thesis/PersonalityPrediction/Datasets/will_label_20000.csv\")\n","\n"],"metadata":{"id":"KujXzreoWW9G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4vlpZ__HLY3U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Terminated"],"metadata":{"id":"a3nxRVMqLZ3A"}}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO6TqJg3k/XgEj7w8CR1F9E"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}